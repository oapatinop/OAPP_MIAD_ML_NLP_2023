{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image info](https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/banner_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aprendizaje por transferencia\n",
    "\n",
    "En este notebook usará  modelos previamente entrenados para la detección de rostros y la predicción de edad y genero, es decir usará aprendizaje por transferencia o *transfer learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones Generales\n",
    "\n",
    "Este notebook usará inicialmente la librería [MTCNN](https://github.com/ipazc/mtcnn) para detectar rostros en una imagen, y posteriormente reutilizará el modelo VGGFace para predecir la edad y el genero de los rostros. En el siguente paper puede conocer más detalles del modelo VGGFace: *Qawaqneh, Z., Mallouh, A. A., & Barkana, B. D. (2017). Deep convolutional neural network for age estimation based on VGG-face model. arXiv preprint arXiv:1709.01664.*. https://arxiv.org/abs/1709.01664\n",
    "  \n",
    "Para realizar la actividad, solo siga las indicaciones asociadas a cada celda del notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importar imagenes y librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "from matplotlib import pyplot\n",
    "from PIL import Image\n",
    "from numpy import asarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/face_1.jpeg\"\n",
    "pixels = pyplot.imread(filename, format='jpeg')\n",
    "pyplot.imshow(pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/face_2.jpeg\"\n",
    "pixels = pyplot.imread(filename, format='jpeg')\n",
    "pyplot.imshow(pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraer rostro de una imagen\n",
    "\n",
    "Para ejecutar esta sección del código debe instalar la librería MTCNN con el comando *!pip install mtcnn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación detector de rostros\n",
    "from mtcnn.mtcnn import MTCNN\n",
    "\n",
    "# Definición detector de rostros\n",
    "detector = MTCNN()\n",
    "results = detector.detect_faces(pixels)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer rostros\n",
    "def get_face(img):\n",
    "    # Carga de imagen\n",
    "    pixels = pyplot.imread(img, format='jpeg')\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # Extracción del rosto\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # Reescalar imagen a tamaño específico\n",
    "    required_size=(224, 224)\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = asarray(image)\n",
    "    # Retornar rostro\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1 = get_face(\"https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/face_1.jpeg\")\n",
    "pyplot.imshow(img1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img2 = get_face(\"https://raw.githubusercontent.com/albahnsen/MIAD_ML_and_NLP/main/images/face_2.jpeg\")\n",
    "pyplot.imshow(img2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generar embbeding (vector) para cada rostro\n",
    "\n",
    "Para ejecutar esta sección del código deben instalar la siguiente librería *!pip install keras_vggface*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras_vggface.vggface import VGGFace\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si al instalar la librería sigue presentando algun error al cargarla, por favor correr el siguiente código en una celda adicional.\n",
    "<code>\n",
    "import sys\n",
    "path = '\\\\'.join(sys.executable.split('\\\\')[:-1])\n",
    "filename = path + \"\\\\lib\\\\site-packages\\\\keras_vggface\\\\models.py\"\n",
    "text = open(filename).read()\n",
    "open(filename, \"w+\").write(text.replace('keras.engine.topology', 'tensorflow.keras.utils'))\n",
    "</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición modelo vggface\n",
    "model = VGGFace(model='resnet50')\n",
    "\n",
    "# impresión de tamaño de inputs y outputs\n",
    "print('Inputs: %s' % model.inputs)\n",
    "print('Outputs: %s' % model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción embedding del modelo para rostro 1\n",
    "yhat1 = model.predict(img1[np.newaxis,:,:,:])\n",
    "print(yhat1.shape)\n",
    "yhat1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicción embedding del modelo para rostro 2\n",
    "yhat2 = model.predict(img2[np.newaxis,:,:,:])\n",
    "print(yhat2.shape)\n",
    "yhat2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predecir genero y edad\n",
    "\n",
    "Para ejecutar esta sección del código debe instalar la librería omegaconf con el comando *!pip install omegaconf*. Adicionalmente, debe descargar un archivo tipo '.hdf5' donde se encuentran los pesos calibrados del modelo preentrenado, esto se realiza con el código de la siguiente celda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# Definir dirección URL donde se encuentra el archivo \n",
    "remote_url = 'https://github.com/yu4u/age-gender-estimation/releases/download/v0.6/EfficientNetB3_224_weights.11-3.44.hdf5'\n",
    "# Definir nombre del archivo\n",
    "local_file = 'datasets/EfficientNetB3_224_weights.11-3.44.hdf5'\n",
    "# Hacer requerimiento con la función request.get() y guardar archivo\n",
    "data = requests.get(remote_url)\n",
    "with open(local_file, 'wb')as file:\n",
    "    file.write(data.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación librerías\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from omegaconf import OmegaConf\n",
    "from pathlib import Path\n",
    "\n",
    "WEIGHTS_FILE = 'datasets/EfficientNetB3_224_weights.11-3.44.hdf5'\n",
    "model_name, img_size = Path(WEIGHTS_FILE).stem.split(\"_\")[:2]\n",
    "cfg = OmegaConf.from_dotlist([f\"model.model_name={model_name}\", f\"model.img_size={img_size}\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definición modelo\n",
    "def get_model(cfg):\n",
    "    base_model = getattr(applications, cfg.model.model_name)(\n",
    "        include_top=False,\n",
    "        input_shape=(cfg.model.img_size, cfg.model.img_size, 3),\n",
    "        pooling=\"avg\"\n",
    "    )\n",
    "    features = base_model.output\n",
    "    # Capa adicional para predicción de genero\n",
    "    pred_gender = Dense(units=2, activation=\"softmax\", name=\"pred_gender\")(features)\n",
    "    # Capa adicional para predicción de edad\n",
    "    pred_age = Dense(units=101, activation=\"softmax\", name=\"pred_age\")(features)\n",
    "    model = Model(inputs=base_model.input, outputs=[pred_gender, pred_age])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo con set de parámetros\n",
    "model = get_model(cfg)\n",
    "model.load_weights(WEIGHTS_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciónes del modelo imagen 1\n",
    "results = model.predict(img1[np.newaxis,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones imagen 1\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "predicted_ages = results[1].dot(ages).flatten()\n",
    "predicted_genere = 'hombre' if (results[0][0][1] >= 0.5) else 'mujer'\n",
    "\n",
    "print('Edad imagen 1: ', predicted_ages)\n",
    "print('Genero imagen 1: ', predicted_genere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciónes del modelo imagen 2\n",
    "results = model.predict(img2[np.newaxis,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicciones imagen 2\n",
    "ages = np.arange(0, 101).reshape(101, 1)\n",
    "predicted_ages = results[1].dot(ages).flatten()\n",
    "predicted_genere = 'hombre' if (results[0][0][1] >= 0.5) else 'mujer'\n",
    "\n",
    "print('Edad imagen 2: ', predicted_ages)\n",
    "print('Genero imagen 2: ', predicted_genere)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Cargar el modelo pre-entrenado y el tokenizador de BERT\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=10)  # Aquí se asume que hay 10 géneros de películas\n",
    "\n",
    "# Sinopsis de ejemplo\n",
    "sinopsis = \"Un grupo de amigos se aventura en una emocionante búsqueda para encontrar un tesoro escondido.\"\n",
    "\n",
    "# Preprocesamiento de la sinopsis\n",
    "tokens = tokenizer.encode_plus(\n",
    "    sinopsis,\n",
    "    None,\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,  # Ajusta la longitud máxima según tus necesidades\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_token_type_ids=False,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'  # Devuelve tensores de PyTorch\n",
    ")\n",
    "\n",
    "# Pasar los tensores a través del modelo\n",
    "outputs = model(input_ids=tokens['input_ids'], attention_mask=tokens['attention_mask'])\n",
    "\n",
    "# Obtener las predicciones de género\n",
    "predictions = outputs.logits.argmax(dim=1).item()\n",
    "\n",
    "# Mapeo de los índices de género a etiquetas de texto\n",
    "genre_labels = ['Acción', 'Aventura', 'Comedia', 'Drama', 'Fantasía', 'Horror', 'Romance', 'Ciencia ficción', 'Suspense', 'Western']\n",
    "predicted_genre = genre_labels[predictions]\n",
    "\n",
    "# Imprimir el género predicho\n",
    "print(\"Género predicho:\", predicted_genre)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este ejemplo, se utiliza el modelo BERT base sin caja y se carga el tokenizador y el modelo pre-entrenado utilizando BertTokenizer.from_pretrained() y BertForSequenceClassification.from_pretrained(), respectivamente. Luego, se pasa la sinopsis a través del tokenizador para obtener los tokens de entrada. A continuación, se utilizan los tensores de entrada para obtener las predicciones de género mediante model(). Finalmente, se mapea el índice de género predicho a una etiqueta de texto utilizando una lista genre_labels.\n",
    "\n",
    "Recuerda que este es solo un ejemplo básico y puedes ajustar y ampliar el código según tus necesidades específicas. Además, ten en cuenta que este código asume que tienes un modelo pre-entrenado de clasificación de géneros con 10 etiquetas y que has instalado la biblioteca Transformers de Hugging Face en tu entorno Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
